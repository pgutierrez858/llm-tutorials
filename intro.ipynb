{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65bc07e57106b041",
   "metadata": {},
   "source": [
    " # Primeros pasos con {guidance}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac55996-7146-4132-9d85-206b4d4af072",
   "metadata": {},
   "source": [
    "**¡OJO!** lee el contenido de las siguientes celdas de código antes de ejecutar nada. Si intentas cargar un modelo demasiado grande para tu equipo es posible que se te acabe congelando o ralentizando de manera significativa. Abajo hay alternativas con distintos grandos de exigencia.\n",
    "\n",
    "**¡OJO 2!** Todo el setup del proyecto está hecho para poder ejecutarlo con CUDA, lo cual puede o no estar soportado por tu equipo. Si no es el caso, basta con modificar el fichero de Poetry para que se instale la versión CPU en vez de la GPU, y NO utilizar modelos cuantizados, pues estos dependen de estar corriendo todo en sus versiones GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4c13c39-9814-4baa-b490-d9e22d3f142f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU count: 1\n",
      "GPU name: NVIDIA GeForce RTX 2060\n",
      "Total GPU memory: 6.00 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"GPU count:\", torch.cuda.device_count())\n",
    "print(\"GPU name:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    total_memory = torch.cuda.get_device_properties(0).total_memory\n",
    "    print(f\"Total GPU memory: {total_memory / (1024 ** 3):.2f} GB\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9411b35123845892",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-04T16:40:39.036414Z",
     "start_time": "2025-07-04T16:38:54.478722Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot use verbose=True in this context (probably CoLab). See https://github.com/abetlen/llama-cpp-python/issues/729\n",
      "llama_model_loader: loaded meta data with 27 key-value pairs and 291 tensors from Meta-Llama-3-8B-Instruct.Q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Models\n",
      "llama_model_loader: - kv   3:                         general.size_label str              = 8.0B\n",
      "llama_model_loader: - kv   4:                            general.license str              = llama3\n",
      "llama_model_loader: - kv   5:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\n",
      "llama_model_loader: - kv   6:                          general.languages arr[str,1]       = [\"en\"]\n",
      "llama_model_loader: - kv   7:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   8:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   9:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  10:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  11:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  12:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  13:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  14:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  15:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  16:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  17:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  18:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  19:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  20:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  21:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
      "llama_model_loader: - kv  26:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_0:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q4_0\n",
      "print_info: file size   = 4.33 GiB (4.64 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 2\n",
      "load: control token: 128095 '<|reserved_special_token_90|>' is not marked as EOG\n",
      "load: control token: 128188 '<|reserved_special_token_183|>' is not marked as EOG\n",
      "load: control token: 128127 '<|reserved_special_token_122|>' is not marked as EOG\n",
      "load: control token: 128116 '<|reserved_special_token_111|>' is not marked as EOG\n",
      "load: control token: 128133 '<|reserved_special_token_128|>' is not marked as EOG\n",
      "load: control token: 128152 '<|reserved_special_token_147|>' is not marked as EOG\n",
      "load: control token: 128193 '<|reserved_special_token_188|>' is not marked as EOG\n",
      "load: control token: 128098 '<|reserved_special_token_93|>' is not marked as EOG\n",
      "load: control token: 128135 '<|reserved_special_token_130|>' is not marked as EOG\n",
      "load: control token: 128178 '<|reserved_special_token_173|>' is not marked as EOG\n",
      "load: control token: 128031 '<|reserved_special_token_26|>' is not marked as EOG\n",
      "load: control token: 128206 '<|reserved_special_token_201|>' is not marked as EOG\n",
      "load: control token: 128028 '<|reserved_special_token_23|>' is not marked as EOG\n",
      "load: control token: 128047 '<|reserved_special_token_42|>' is not marked as EOG\n",
      "load: control token: 128241 '<|reserved_special_token_236|>' is not marked as EOG\n",
      "load: control token: 128145 '<|reserved_special_token_140|>' is not marked as EOG\n",
      "load: control token: 128195 '<|reserved_special_token_190|>' is not marked as EOG\n",
      "load: control token: 128226 '<|reserved_special_token_221|>' is not marked as EOG\n",
      "load: control token: 128162 '<|reserved_special_token_157|>' is not marked as EOG\n",
      "load: control token: 128243 '<|reserved_special_token_238|>' is not marked as EOG\n",
      "load: control token: 128014 '<|reserved_special_token_9|>' is not marked as EOG\n",
      "load: control token: 128213 '<|reserved_special_token_208|>' is not marked as EOG\n",
      "load: control token: 128158 '<|reserved_special_token_153|>' is not marked as EOG\n",
      "load: control token: 128221 '<|reserved_special_token_216|>' is not marked as EOG\n",
      "load: control token: 128079 '<|reserved_special_token_74|>' is not marked as EOG\n",
      "load: control token: 128246 '<|reserved_special_token_241|>' is not marked as EOG\n",
      "load: control token: 128104 '<|reserved_special_token_99|>' is not marked as EOG\n",
      "load: control token: 128076 '<|reserved_special_token_71|>' is not marked as EOG\n",
      "load: control token: 128222 '<|reserved_special_token_217|>' is not marked as EOG\n",
      "load: control token: 128172 '<|reserved_special_token_167|>' is not marked as EOG\n",
      "load: control token: 128220 '<|reserved_special_token_215|>' is not marked as EOG\n",
      "load: control token: 128179 '<|reserved_special_token_174|>' is not marked as EOG\n",
      "load: control token: 128065 '<|reserved_special_token_60|>' is not marked as EOG\n",
      "load: control token: 128249 '<|reserved_special_token_244|>' is not marked as EOG\n",
      "load: control token: 128175 '<|reserved_special_token_170|>' is not marked as EOG\n",
      "load: control token: 128218 '<|reserved_special_token_213|>' is not marked as EOG\n",
      "load: control token: 128049 '<|reserved_special_token_44|>' is not marked as EOG\n",
      "load: control token: 128119 '<|reserved_special_token_114|>' is not marked as EOG\n",
      "load: control token: 128148 '<|reserved_special_token_143|>' is not marked as EOG\n",
      "load: control token: 128118 '<|reserved_special_token_113|>' is not marked as EOG\n",
      "load: control token: 128155 '<|reserved_special_token_150|>' is not marked as EOG\n",
      "load: control token: 128093 '<|reserved_special_token_88|>' is not marked as EOG\n",
      "load: control token: 128087 '<|reserved_special_token_82|>' is not marked as EOG\n",
      "load: control token: 128235 '<|reserved_special_token_230|>' is not marked as EOG\n",
      "load: control token: 128136 '<|reserved_special_token_131|>' is not marked as EOG\n",
      "load: control token: 128173 '<|reserved_special_token_168|>' is not marked as EOG\n",
      "load: control token: 128074 '<|reserved_special_token_69|>' is not marked as EOG\n",
      "load: control token: 128211 '<|reserved_special_token_206|>' is not marked as EOG\n",
      "load: control token: 128168 '<|reserved_special_token_163|>' is not marked as EOG\n",
      "load: control token: 128109 '<|reserved_special_token_104|>' is not marked as EOG\n",
      "load: control token: 128177 '<|reserved_special_token_172|>' is not marked as EOG\n",
      "load: control token: 128057 '<|reserved_special_token_52|>' is not marked as EOG\n",
      "load: control token: 128000 '<|begin_of_text|>' is not marked as EOG\n",
      "load: control token: 128149 '<|reserved_special_token_144|>' is not marked as EOG\n",
      "load: control token: 128113 '<|reserved_special_token_108|>' is not marked as EOG\n",
      "load: control token: 128069 '<|reserved_special_token_64|>' is not marked as EOG\n",
      "load: control token: 128056 '<|reserved_special_token_51|>' is not marked as EOG\n",
      "load: control token: 128091 '<|reserved_special_token_86|>' is not marked as EOG\n",
      "load: control token: 128184 '<|reserved_special_token_179|>' is not marked as EOG\n",
      "load: control token: 128100 '<|reserved_special_token_95|>' is not marked as EOG\n",
      "load: control token: 128124 '<|reserved_special_token_119|>' is not marked as EOG\n",
      "load: control token: 128020 '<|reserved_special_token_15|>' is not marked as EOG\n",
      "load: control token: 128034 '<|reserved_special_token_29|>' is not marked as EOG\n",
      "load: control token: 128225 '<|reserved_special_token_220|>' is not marked as EOG\n",
      "load: control token: 128002 '<|reserved_special_token_0|>' is not marked as EOG\n",
      "load: control token: 128006 '<|start_header_id|>' is not marked as EOG\n",
      "load: control token: 128088 '<|reserved_special_token_83|>' is not marked as EOG\n",
      "load: control token: 128041 '<|reserved_special_token_36|>' is not marked as EOG\n",
      "load: control token: 128215 '<|reserved_special_token_210|>' is not marked as EOG\n",
      "load: control token: 128208 '<|reserved_special_token_203|>' is not marked as EOG\n",
      "load: control token: 128070 '<|reserved_special_token_65|>' is not marked as EOG\n",
      "load: control token: 128165 '<|reserved_special_token_160|>' is not marked as EOG\n",
      "load: control token: 128180 '<|reserved_special_token_175|>' is not marked as EOG\n",
      "load: control token: 128231 '<|reserved_special_token_226|>' is not marked as EOG\n",
      "load: control token: 128232 '<|reserved_special_token_227|>' is not marked as EOG\n",
      "load: control token: 128064 '<|reserved_special_token_59|>' is not marked as EOG\n",
      "load: control token: 128036 '<|reserved_special_token_31|>' is not marked as EOG\n",
      "load: control token: 128103 '<|reserved_special_token_98|>' is not marked as EOG\n",
      "load: control token: 128247 '<|reserved_special_token_242|>' is not marked as EOG\n",
      "load: control token: 128170 '<|reserved_special_token_165|>' is not marked as EOG\n",
      "load: control token: 128123 '<|reserved_special_token_118|>' is not marked as EOG\n",
      "load: control token: 128044 '<|reserved_special_token_39|>' is not marked as EOG\n",
      "load: control token: 128237 '<|reserved_special_token_232|>' is not marked as EOG\n",
      "load: control token: 128042 '<|reserved_special_token_37|>' is not marked as EOG\n",
      "load: control token: 128192 '<|reserved_special_token_187|>' is not marked as EOG\n",
      "load: control token: 128075 '<|reserved_special_token_70|>' is not marked as EOG\n",
      "load: control token: 128134 '<|reserved_special_token_129|>' is not marked as EOG\n",
      "load: control token: 128183 '<|reserved_special_token_178|>' is not marked as EOG\n",
      "load: control token: 128045 '<|reserved_special_token_40|>' is not marked as EOG\n",
      "load: control token: 128073 '<|reserved_special_token_68|>' is not marked as EOG\n",
      "load: control token: 128026 '<|reserved_special_token_21|>' is not marked as EOG\n",
      "load: control token: 128010 '<|reserved_special_token_5|>' is not marked as EOG\n",
      "load: control token: 128194 '<|reserved_special_token_189|>' is not marked as EOG\n",
      "load: control token: 128053 '<|reserved_special_token_48|>' is not marked as EOG\n",
      "load: control token: 128120 '<|reserved_special_token_115|>' is not marked as EOG\n",
      "load: control token: 128092 '<|reserved_special_token_87|>' is not marked as EOG\n",
      "load: control token: 128086 '<|reserved_special_token_81|>' is not marked as EOG\n",
      "load: control token: 128054 '<|reserved_special_token_49|>' is not marked as EOG\n",
      "load: control token: 128160 '<|reserved_special_token_155|>' is not marked as EOG\n",
      "load: control token: 128005 '<|reserved_special_token_3|>' is not marked as EOG\n",
      "load: control token: 128050 '<|reserved_special_token_45|>' is not marked as EOG\n",
      "load: control token: 128157 '<|reserved_special_token_152|>' is not marked as EOG\n",
      "load: control token: 128219 '<|reserved_special_token_214|>' is not marked as EOG\n",
      "load: control token: 128032 '<|reserved_special_token_27|>' is not marked as EOG\n",
      "load: control token: 128159 '<|reserved_special_token_154|>' is not marked as EOG\n",
      "load: control token: 128202 '<|reserved_special_token_197|>' is not marked as EOG\n",
      "load: control token: 128106 '<|reserved_special_token_101|>' is not marked as EOG\n",
      "load: control token: 128182 '<|reserved_special_token_177|>' is not marked as EOG\n",
      "load: control token: 128111 '<|reserved_special_token_106|>' is not marked as EOG\n",
      "load: control token: 128156 '<|reserved_special_token_151|>' is not marked as EOG\n",
      "load: control token: 128176 '<|reserved_special_token_171|>' is not marked as EOG\n",
      "load: control token: 128112 '<|reserved_special_token_107|>' is not marked as EOG\n",
      "load: control token: 128084 '<|reserved_special_token_79|>' is not marked as EOG\n",
      "load: control token: 128110 '<|reserved_special_token_105|>' is not marked as EOG\n",
      "load: control token: 128051 '<|reserved_special_token_46|>' is not marked as EOG\n",
      "load: control token: 128027 '<|reserved_special_token_22|>' is not marked as EOG\n",
      "load: control token: 128167 '<|reserved_special_token_162|>' is not marked as EOG\n",
      "load: control token: 128008 '<|reserved_special_token_4|>' is not marked as EOG\n",
      "load: control token: 128061 '<|reserved_special_token_56|>' is not marked as EOG\n",
      "load: control token: 128115 '<|reserved_special_token_110|>' is not marked as EOG\n",
      "load: control token: 128203 '<|reserved_special_token_198|>' is not marked as EOG\n",
      "load: control token: 128096 '<|reserved_special_token_91|>' is not marked as EOG\n",
      "load: control token: 128130 '<|reserved_special_token_125|>' is not marked as EOG\n",
      "load: control token: 128187 '<|reserved_special_token_182|>' is not marked as EOG\n",
      "load: control token: 128094 '<|reserved_special_token_89|>' is not marked as EOG\n",
      "load: control token: 128083 '<|reserved_special_token_78|>' is not marked as EOG\n",
      "load: control token: 128117 '<|reserved_special_token_112|>' is not marked as EOG\n",
      "load: control token: 128190 '<|reserved_special_token_185|>' is not marked as EOG\n",
      "load: control token: 128046 '<|reserved_special_token_41|>' is not marked as EOG\n",
      "load: control token: 128239 '<|reserved_special_token_234|>' is not marked as EOG\n",
      "load: control token: 128139 '<|reserved_special_token_134|>' is not marked as EOG\n",
      "load: control token: 128185 '<|reserved_special_token_180|>' is not marked as EOG\n",
      "load: control token: 128141 '<|reserved_special_token_136|>' is not marked as EOG\n",
      "load: control token: 128244 '<|reserved_special_token_239|>' is not marked as EOG\n",
      "load: control token: 128062 '<|reserved_special_token_57|>' is not marked as EOG\n",
      "load: control token: 128114 '<|reserved_special_token_109|>' is not marked as EOG\n",
      "load: control token: 128030 '<|reserved_special_token_25|>' is not marked as EOG\n",
      "load: control token: 128181 '<|reserved_special_token_176|>' is not marked as EOG\n",
      "load: control token: 128037 '<|reserved_special_token_32|>' is not marked as EOG\n",
      "load: control token: 128201 '<|reserved_special_token_196|>' is not marked as EOG\n",
      "load: control token: 128207 '<|reserved_special_token_202|>' is not marked as EOG\n",
      "load: control token: 128242 '<|reserved_special_token_237|>' is not marked as EOG\n",
      "load: control token: 128132 '<|reserved_special_token_127|>' is not marked as EOG\n",
      "load: control token: 128068 '<|reserved_special_token_63|>' is not marked as EOG\n",
      "load: control token: 128150 '<|reserved_special_token_145|>' is not marked as EOG\n",
      "load: control token: 128191 '<|reserved_special_token_186|>' is not marked as EOG\n",
      "load: control token: 128174 '<|reserved_special_token_169|>' is not marked as EOG\n",
      "load: control token: 128233 '<|reserved_special_token_228|>' is not marked as EOG\n",
      "load: control token: 128245 '<|reserved_special_token_240|>' is not marked as EOG\n",
      "load: control token: 128238 '<|reserved_special_token_233|>' is not marked as EOG\n",
      "load: control token: 128209 '<|reserved_special_token_204|>' is not marked as EOG\n",
      "load: control token: 128204 '<|reserved_special_token_199|>' is not marked as EOG\n",
      "load: control token: 128003 '<|reserved_special_token_1|>' is not marked as EOG\n",
      "load: control token: 128004 '<|reserved_special_token_2|>' is not marked as EOG\n",
      "load: control token: 128007 '<|end_header_id|>' is not marked as EOG\n",
      "load: control token: 128011 '<|reserved_special_token_6|>' is not marked as EOG\n",
      "load: control token: 128012 '<|reserved_special_token_7|>' is not marked as EOG\n",
      "load: control token: 128013 '<|reserved_special_token_8|>' is not marked as EOG\n",
      "load: control token: 128015 '<|reserved_special_token_10|>' is not marked as EOG\n",
      "load: control token: 128016 '<|reserved_special_token_11|>' is not marked as EOG\n",
      "load: control token: 128017 '<|reserved_special_token_12|>' is not marked as EOG\n",
      "load: control token: 128018 '<|reserved_special_token_13|>' is not marked as EOG\n",
      "load: control token: 128019 '<|reserved_special_token_14|>' is not marked as EOG\n",
      "load: control token: 128021 '<|reserved_special_token_16|>' is not marked as EOG\n",
      "load: control token: 128022 '<|reserved_special_token_17|>' is not marked as EOG\n",
      "load: control token: 128023 '<|reserved_special_token_18|>' is not marked as EOG\n",
      "load: control token: 128024 '<|reserved_special_token_19|>' is not marked as EOG\n",
      "load: control token: 128025 '<|reserved_special_token_20|>' is not marked as EOG\n",
      "load: control token: 128029 '<|reserved_special_token_24|>' is not marked as EOG\n",
      "load: control token: 128033 '<|reserved_special_token_28|>' is not marked as EOG\n",
      "load: control token: 128035 '<|reserved_special_token_30|>' is not marked as EOG\n",
      "load: control token: 128038 '<|reserved_special_token_33|>' is not marked as EOG\n",
      "load: control token: 128039 '<|reserved_special_token_34|>' is not marked as EOG\n",
      "load: control token: 128040 '<|reserved_special_token_35|>' is not marked as EOG\n",
      "load: control token: 128043 '<|reserved_special_token_38|>' is not marked as EOG\n",
      "load: control token: 128048 '<|reserved_special_token_43|>' is not marked as EOG\n",
      "load: control token: 128052 '<|reserved_special_token_47|>' is not marked as EOG\n",
      "load: control token: 128055 '<|reserved_special_token_50|>' is not marked as EOG\n",
      "load: control token: 128058 '<|reserved_special_token_53|>' is not marked as EOG\n",
      "load: control token: 128059 '<|reserved_special_token_54|>' is not marked as EOG\n",
      "load: control token: 128060 '<|reserved_special_token_55|>' is not marked as EOG\n",
      "load: control token: 128063 '<|reserved_special_token_58|>' is not marked as EOG\n",
      "load: control token: 128066 '<|reserved_special_token_61|>' is not marked as EOG\n",
      "load: control token: 128067 '<|reserved_special_token_62|>' is not marked as EOG\n",
      "load: control token: 128071 '<|reserved_special_token_66|>' is not marked as EOG\n",
      "load: control token: 128072 '<|reserved_special_token_67|>' is not marked as EOG\n",
      "load: control token: 128077 '<|reserved_special_token_72|>' is not marked as EOG\n",
      "load: control token: 128078 '<|reserved_special_token_73|>' is not marked as EOG\n",
      "load: control token: 128080 '<|reserved_special_token_75|>' is not marked as EOG\n",
      "load: control token: 128081 '<|reserved_special_token_76|>' is not marked as EOG\n",
      "load: control token: 128082 '<|reserved_special_token_77|>' is not marked as EOG\n",
      "load: control token: 128085 '<|reserved_special_token_80|>' is not marked as EOG\n",
      "load: control token: 128089 '<|reserved_special_token_84|>' is not marked as EOG\n",
      "load: control token: 128090 '<|reserved_special_token_85|>' is not marked as EOG\n",
      "load: control token: 128097 '<|reserved_special_token_92|>' is not marked as EOG\n",
      "load: control token: 128099 '<|reserved_special_token_94|>' is not marked as EOG\n",
      "load: control token: 128101 '<|reserved_special_token_96|>' is not marked as EOG\n",
      "load: control token: 128102 '<|reserved_special_token_97|>' is not marked as EOG\n",
      "load: control token: 128105 '<|reserved_special_token_100|>' is not marked as EOG\n",
      "load: control token: 128107 '<|reserved_special_token_102|>' is not marked as EOG\n",
      "load: control token: 128108 '<|reserved_special_token_103|>' is not marked as EOG\n",
      "load: control token: 128121 '<|reserved_special_token_116|>' is not marked as EOG\n",
      "load: control token: 128122 '<|reserved_special_token_117|>' is not marked as EOG\n",
      "load: control token: 128125 '<|reserved_special_token_120|>' is not marked as EOG\n",
      "load: control token: 128126 '<|reserved_special_token_121|>' is not marked as EOG\n",
      "load: control token: 128128 '<|reserved_special_token_123|>' is not marked as EOG\n",
      "load: control token: 128129 '<|reserved_special_token_124|>' is not marked as EOG\n",
      "load: control token: 128131 '<|reserved_special_token_126|>' is not marked as EOG\n",
      "load: control token: 128137 '<|reserved_special_token_132|>' is not marked as EOG\n",
      "load: control token: 128138 '<|reserved_special_token_133|>' is not marked as EOG\n",
      "load: control token: 128140 '<|reserved_special_token_135|>' is not marked as EOG\n",
      "load: control token: 128142 '<|reserved_special_token_137|>' is not marked as EOG\n",
      "load: control token: 128143 '<|reserved_special_token_138|>' is not marked as EOG\n",
      "load: control token: 128144 '<|reserved_special_token_139|>' is not marked as EOG\n",
      "load: control token: 128146 '<|reserved_special_token_141|>' is not marked as EOG\n",
      "load: control token: 128147 '<|reserved_special_token_142|>' is not marked as EOG\n",
      "load: control token: 128151 '<|reserved_special_token_146|>' is not marked as EOG\n",
      "load: control token: 128153 '<|reserved_special_token_148|>' is not marked as EOG\n",
      "load: control token: 128154 '<|reserved_special_token_149|>' is not marked as EOG\n",
      "load: control token: 128161 '<|reserved_special_token_156|>' is not marked as EOG\n",
      "load: control token: 128163 '<|reserved_special_token_158|>' is not marked as EOG\n",
      "load: control token: 128164 '<|reserved_special_token_159|>' is not marked as EOG\n",
      "load: control token: 128166 '<|reserved_special_token_161|>' is not marked as EOG\n",
      "load: control token: 128169 '<|reserved_special_token_164|>' is not marked as EOG\n",
      "load: control token: 128171 '<|reserved_special_token_166|>' is not marked as EOG\n",
      "load: control token: 128186 '<|reserved_special_token_181|>' is not marked as EOG\n",
      "load: control token: 128189 '<|reserved_special_token_184|>' is not marked as EOG\n",
      "load: control token: 128196 '<|reserved_special_token_191|>' is not marked as EOG\n",
      "load: control token: 128197 '<|reserved_special_token_192|>' is not marked as EOG\n",
      "load: control token: 128198 '<|reserved_special_token_193|>' is not marked as EOG\n",
      "load: control token: 128199 '<|reserved_special_token_194|>' is not marked as EOG\n",
      "load: control token: 128200 '<|reserved_special_token_195|>' is not marked as EOG\n",
      "load: control token: 128205 '<|reserved_special_token_200|>' is not marked as EOG\n",
      "load: control token: 128210 '<|reserved_special_token_205|>' is not marked as EOG\n",
      "load: control token: 128212 '<|reserved_special_token_207|>' is not marked as EOG\n",
      "load: control token: 128214 '<|reserved_special_token_209|>' is not marked as EOG\n",
      "load: control token: 128216 '<|reserved_special_token_211|>' is not marked as EOG\n",
      "load: control token: 128217 '<|reserved_special_token_212|>' is not marked as EOG\n",
      "load: control token: 128223 '<|reserved_special_token_218|>' is not marked as EOG\n",
      "load: control token: 128224 '<|reserved_special_token_219|>' is not marked as EOG\n",
      "load: control token: 128227 '<|reserved_special_token_222|>' is not marked as EOG\n",
      "load: control token: 128228 '<|reserved_special_token_223|>' is not marked as EOG\n",
      "load: control token: 128229 '<|reserved_special_token_224|>' is not marked as EOG\n",
      "load: control token: 128230 '<|reserved_special_token_225|>' is not marked as EOG\n",
      "load: control token: 128234 '<|reserved_special_token_229|>' is not marked as EOG\n",
      "load: control token: 128236 '<|reserved_special_token_231|>' is not marked as EOG\n",
      "load: control token: 128240 '<|reserved_special_token_235|>' is not marked as EOG\n",
      "load: control token: 128248 '<|reserved_special_token_243|>' is not marked as EOG\n",
      "load: control token: 128250 '<|reserved_special_token_245|>' is not marked as EOG\n",
      "load: control token: 128251 '<|reserved_special_token_246|>' is not marked as EOG\n",
      "load: control token: 128252 '<|reserved_special_token_247|>' is not marked as EOG\n",
      "load: control token: 128253 '<|reserved_special_token_248|>' is not marked as EOG\n",
      "load: control token: 128254 '<|reserved_special_token_249|>' is not marked as EOG\n",
      "load: control token: 128255 '<|reserved_special_token_250|>' is not marked as EOG\n",
      "load: special tokens cache size = 256\n",
      "load: token to piece cache size = 0.8000 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 8192\n",
      "print_info: n_embd           = 4096\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: is_swa_any       = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 4\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 14336\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 500000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 8192\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: model type       = 8B\n",
      "print_info: model params     = 8.03 B\n",
      "print_info: general.name     = Models\n",
      "print_info: vocab type       = BPE\n",
      "print_info: n_vocab          = 128256\n",
      "print_info: n_merges         = 280147\n",
      "print_info: BOS token        = 128000 '<|begin_of_text|>'\n",
      "print_info: EOS token        = 128009 '<|eot_id|>'\n",
      "print_info: EOT token        = 128009 '<|eot_id|>'\n",
      "print_info: LF token         = 198 'Ċ'\n",
      "print_info: EOG token        = 128001 '<|end_of_text|>'\n",
      "print_info: EOG token        = 128009 '<|eot_id|>'\n",
      "print_info: max token length = 256\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  12 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  14 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  16 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  17 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  18 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  19 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  20 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  21 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  22 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  23 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  24 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  25 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  26 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  27 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  28 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  29 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  30 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  31 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  32 assigned to device CPU, is_swa = 0\n",
      "load_tensors: tensor 'token_embd.weight' (q4_0) (and 66 others) cannot be used with preferred buffer type CPU_REPACK, using CPU instead\n",
      "load_tensors:   CPU_REPACK model buffer size =  3744.00 MiB\n",
      "load_tensors:   CPU_Mapped model buffer size =  4437.80 MiB\n",
      "repack: repack tensor blk.0.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.0.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.0.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.0.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.0.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.0.ffn_down.weight with q4_0_8x8\n",
      "repack: repack tensor blk.0.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.1.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.1.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.1.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.1.attn_output.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.1.ffn_gate.weight with q4_0_8x8\n",
      "repack: repack tensor blk.1.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.1.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.2.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.2.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.2.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.2.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.2.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.2.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.2.ffn_up.weight with q4_0_8x8\n",
      "repack: repack tensor blk.3.attn_q.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.3.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.3.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.3.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.3.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.3.ffn_down.weight with q4_0_8x8\n",
      "repack: repack tensor blk.3.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.4.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.4.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.4.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.4.attn_output.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.4.ffn_gate.weight with q4_0_8x8\n",
      "repack: repack tensor blk.4.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.4.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.5.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.5.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.5.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.5.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.5.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.5.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.5.ffn_up.weight with q4_0_8x8\n",
      "repack: repack tensor blk.6.attn_q.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.6.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.6.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.6.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.6.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.6.ffn_down.weight with q4_0_8x8\n",
      "repack: repack tensor blk.6.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.7.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.7.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.7.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.7.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.7.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.7.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.7.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.8.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.8.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.8.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.8.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.8.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.8.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.8.ffn_up.weight with q4_0_8x8\n",
      "repack: repack tensor blk.9.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.9.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.9.attn_v.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.9.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.9.ffn_gate.weight with q4_0_8x8\n",
      "repack: repack tensor blk.9.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.9.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.10.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.10.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.10.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.10.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.10.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.10.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.10.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.11.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.11.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.11.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.11.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.11.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.11.ffn_down.weight with q4_0_8x8\n",
      "repack: repack tensor blk.11.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.12.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.12.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.12.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.12.attn_output.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.12.ffn_gate.weight with q4_0_8x8\n",
      "repack: repack tensor blk.12.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.12.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.13.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.13.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.13.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.13.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.13.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.13.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.13.ffn_up.weight with q4_0_8x8\n",
      "repack: repack tensor blk.14.attn_q.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.14.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.14.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.14.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.14.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.14.ffn_down.weight with q4_0_8x8\n",
      "repack: repack tensor blk.14.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.15.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.15.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.15.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.15.attn_output.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.15.ffn_gate.weight with q4_0_8x8\n",
      "repack: repack tensor blk.15.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.15.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.16.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.16.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.16.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.16.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.16.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.16.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.16.ffn_up.weight with q4_0_8x8\n",
      "repack: repack tensor blk.17.attn_q.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.17.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.17.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.17.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.17.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.17.ffn_down.weight with q4_0_8x8\n",
      "repack: repack tensor blk.17.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.18.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.18.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.18.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.18.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.18.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.18.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.18.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.19.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.19.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.19.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.19.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.19.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.19.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.19.ffn_up.weight with q4_0_8x8\n",
      "repack: repack tensor blk.20.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.20.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.20.attn_v.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.20.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.20.ffn_gate.weight with q4_0_8x8\n",
      "repack: repack tensor blk.20.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.20.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.21.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.21.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.21.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.21.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.21.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.21.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.21.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.22.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.22.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.22.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.22.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.22.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.22.ffn_down.weight with q4_0_8x8\n",
      "repack: repack tensor blk.22.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.23.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.23.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.23.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.23.attn_output.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.23.ffn_gate.weight with q4_0_8x8\n",
      "repack: repack tensor blk.23.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.23.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.24.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.24.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.24.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.24.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.24.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.24.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.24.ffn_up.weight with q4_0_8x8\n",
      "repack: repack tensor blk.25.attn_q.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.25.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.25.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.25.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.25.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.25.ffn_down.weight with q4_0_8x8\n",
      "repack: repack tensor blk.25.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.26.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.26.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.26.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.26.attn_output.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.26.ffn_gate.weight with q4_0_8x8\n",
      "repack: repack tensor blk.26.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.26.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.27.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.27.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.27.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.27.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.27.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.27.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.27.ffn_up.weight with q4_0_8x8\n",
      "repack: repack tensor blk.28.attn_q.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.28.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.28.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.28.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.28.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.28.ffn_down.weight with q4_0_8x8\n",
      "repack: repack tensor blk.28.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.29.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.29.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.29.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.29.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.29.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.29.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.29.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.30.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.30.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.30.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.30.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.30.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.30.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.30.ffn_up.weight with q4_0_8x8\n",
      "repack: repack tensor blk.31.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.31.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.31.attn_v.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.31.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.31.ffn_gate.weight with q4_0_8x8\n",
      "repack: repack tensor blk.31.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.31.ffn_up.weight with q4_0_8x8\n",
      "....\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 8192\n",
      "llama_context: n_ctx_per_seq = 8192\n",
      "llama_context: n_batch       = 512\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: freq_base     = 500000.0\n",
      "llama_context: freq_scale    = 1\n",
      "set_abort_callback: call\n",
      "llama_context:        CPU  output buffer size =     0.49 MiB\n",
      "create_memory: n_ctx = 8192 (padded)\n",
      "llama_kv_cache_unified: layer   0: dev = CPU\n",
      "llama_kv_cache_unified: layer   1: dev = CPU\n",
      "llama_kv_cache_unified: layer   2: dev = CPU\n",
      "llama_kv_cache_unified: layer   3: dev = CPU\n",
      "llama_kv_cache_unified: layer   4: dev = CPU\n",
      "llama_kv_cache_unified: layer   5: dev = CPU\n",
      "llama_kv_cache_unified: layer   6: dev = CPU\n",
      "llama_kv_cache_unified: layer   7: dev = CPU\n",
      "llama_kv_cache_unified: layer   8: dev = CPU\n",
      "llama_kv_cache_unified: layer   9: dev = CPU\n",
      "llama_kv_cache_unified: layer  10: dev = CPU\n",
      "llama_kv_cache_unified: layer  11: dev = CPU\n",
      "llama_kv_cache_unified: layer  12: dev = CPU\n",
      "llama_kv_cache_unified: layer  13: dev = CPU\n",
      "llama_kv_cache_unified: layer  14: dev = CPU\n",
      "llama_kv_cache_unified: layer  15: dev = CPU\n",
      "llama_kv_cache_unified: layer  16: dev = CPU\n",
      "llama_kv_cache_unified: layer  17: dev = CPU\n",
      "llama_kv_cache_unified: layer  18: dev = CPU\n",
      "llama_kv_cache_unified: layer  19: dev = CPU\n",
      "llama_kv_cache_unified: layer  20: dev = CPU\n",
      "llama_kv_cache_unified: layer  21: dev = CPU\n",
      "llama_kv_cache_unified: layer  22: dev = CPU\n",
      "llama_kv_cache_unified: layer  23: dev = CPU\n",
      "llama_kv_cache_unified: layer  24: dev = CPU\n",
      "llama_kv_cache_unified: layer  25: dev = CPU\n",
      "llama_kv_cache_unified: layer  26: dev = CPU\n",
      "llama_kv_cache_unified: layer  27: dev = CPU\n",
      "llama_kv_cache_unified: layer  28: dev = CPU\n",
      "llama_kv_cache_unified: layer  29: dev = CPU\n",
      "llama_kv_cache_unified: layer  30: dev = CPU\n",
      "llama_kv_cache_unified: layer  31: dev = CPU\n",
      "llama_kv_cache_unified:        CPU KV buffer size =  1024.00 MiB\n",
      "llama_kv_cache_unified: size = 1024.00 MiB (  8192 cells,  32 layers,  1 seqs), K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
      "llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n",
      "llama_context: enumerating backends\n",
      "llama_context: backend_ptrs.size() = 1\n",
      "llama_context: max_nodes = 65536\n",
      "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "llama_context:        CPU compute buffer size =   560.01 MiB\n",
      "llama_context: graph nodes  = 1158\n",
      "llama_context: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n",
      "Model metadata: {'general.name': 'Models', 'general.architecture': 'llama', 'general.type': 'model', 'general.size_label': '8.0B', 'general.license': 'llama3', 'llama.block_count': '32', 'llama.context_length': '8192', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '128009', 'general.file_type': '2', 'llama.attention.head_count_kv': '8', 'llama.rope.freq_base': '500000.000000', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.vocab_size': '128256', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.model': 'gpt2', 'tokenizer.ggml.pre': 'llama-bpe', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '128000', 'tokenizer.chat_template': \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\"}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Guessed chat format: llama-3\n"
     ]
    }
   ],
   "source": [
    "from guidance import system, user, assistant, gen\n",
    "from guidance.models import LlamaCpp\n",
    "from guidance.models import Transformers\n",
    "\n",
    "# Yo personalmente usaré un modelo Llama pequeño de Hugging Face por sencillez y por restricciones de hardware.\n",
    "# Como probablemente cada uno tendrá un hardware muy diferente, dejo algunas opciones abajo como referencia,\n",
    "# ordenadas de menor a mayor uso de recursos:\n",
    "\n",
    "# (1) Modelo MUY limitado, pero que seguramente corra en la mayoría de PCs mínimamente modernos.\n",
    "# llama_lm = Transformers(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "\n",
    "# (2) Modelo Phi 4 Instruct \"cuantizado\". De forma intuitiva, esto es parecido a \"comprimir\" un modelo\n",
    "# existente para reducir su uso de memoria y agilizar la inferencia, intentando sacrificar\n",
    "# el menor rendimiento posible.\n",
    "# En más detalle: \"Quantized models are machine learning models where the numerical precision of\n",
    "# the model's parameters (like weights) has been reduced, typically from higher precision formats like\n",
    "# 32-bit floating-point to lower precision formats like 8-bit integers. This process reduces the model's\n",
    "# size, memory usage, and computational requirements, making it more efficient for deployment, especially on resource-constrained devices\"\n",
    "llama_lm = LlamaCpp(model=\"Meta-Llama-3-8B-Instruct.Q4_0.gguf\", device_map=\"auto\", n_ctx=8192)\n",
    "\n",
    "# (3) Modelo usado en el tutorial de Guidance, sin cuantizar, con un rendimiento muy bueno\n",
    "# pero que puede llegar a congelar constantemente el equipo si no dispones de un PC de gama media-alta.\n",
    "# llama_lm = Transformers(\"microsoft/Phi-4-mini-instruct\")\n",
    "\n",
    "# (4) Otros modelos locales descargados en formato GGUF.\n",
    "# llama_lm = LlamaCpp(model=\"path to model on local drive\")\n",
    "\n",
    "# (5) Se pueden usar otros modelos locales, otros de HuggingFace, o incluso APIs como OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3273d692-43c6-413e-ab15-96121048d943",
   "metadata": {},
   "source": [
    "Con guidance, el proceso de generación pasa a parecerse más a \"concatenar\" cadenas de caracteres. Esencialmente, vamos alternando bloques en los que definimos el rol que interviene en la conversación, y dentro de cada bloque anexamos el texto que queramos insertar en el diálogo. \n",
    "\n",
    "Anexar texto en bloques de usuario tiene el mismo efecto que escribir un prompt en el input de ChatGPT o similares. Pero para invocar verdaderamente al LLM necesitamos hacer uso del método de conveniencia de guidance `gen()`, que cede el control de la conversación al asistente hasta que este genere un token de fin de intervención (dependiente del modelo empleado)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21dbe8f0-8fa7-4d18-b92a-8a585b237b48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05e43cdec80b45e285c1440b8f27b9f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "StitchWidget(initial_height='auto', initial_width='100%', srcdoc='<!doctype html>\\n<html lang=\"en\">\\n<head>\\n …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# IMPORTANTE: los modelos de guidance son *immutables*, por lo que cualquier asignación\n",
    "# de un modelo es una copia:\n",
    "lm = llama_lm\n",
    "\n",
    "# Algo conveniente de guidance son los contextos de utilidad para indicar al LLM qué rol\n",
    "# está diciendo qué en la conversación. Por lo general, en system pondremos todas las instrucciones\n",
    "# de \"administración\", habitualmente para darle información al asistente sobre sus características,\n",
    "# estilo, o cualquier otro dato que afecte a su forma de comunicarse con el usuario.\n",
    "with system():\n",
    "    lm += \"You are a bot that expresses itself in verse\"\n",
    "\n",
    "# Y por otro lado, user y assistant se corresponden con los mensajes generados por el bot y por el usuario\n",
    "# en una conversación, como estamos acostumbrados en sistemas como ChatGPT.\n",
    "with user():\n",
    "    lm += \"Good morning! Who are you?\"\n",
    "\n",
    "# llamada básica al asistente. gen() se puede llamar con muchos parámetros para guiar la generación de forma\n",
    "# más precisa, pero para este ejemplo nos limitamos a la invocación básica que simplemente escribe hasta llegar\n",
    "# a un token de fin de bloque.\n",
    "with assistant():\n",
    "    lm += gen()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdb7340-475b-4587-a4ea-b40a9794fcc6",
   "metadata": {},
   "source": [
    "Fíjate en cómo en el widget de arriba el asistente responde en verso pese a no haber recibido ninguna instrucción del usuario al respecto. Los prompts de sistema, normalmente ocultos para los usuarios, permiten definir este tipo de comportamientos y pueden ser muy útiles para ahorrarnos tiempo en configuración general que aplique a todas las intervenciones del bot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19126778-8bf6-4ece-983d-1880a4619274",
   "metadata": {},
   "source": [
    "Vamos a empezar a añadir más parámetros a la llamada de `gen()`. El siguiente caso de uso más sencillo es ser capaces de almacenar en variables de Python las respuestas del asistente (o partes de ellas). Internamente, cada instancia de un modelo almacena un diccionario clave/valor que puede rellenarse indicando el nombre de la clave a insertar en el diccionario en la llamada a `gen()`, tras lo cual el output de dicha llamada pasará a aparecer como valor en la clave correspondiente. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "702de979-cb35-42eb-9098-2a5167d9140e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91714d20d3d944fb83aa399ce5dc2767",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "StitchWidget(initial_height='auto', initial_width='100%', srcdoc='<!doctype html>\\n<html lang=\"en\">\\n<head>\\n …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lm['trad_eng']='Este es un lápiz. Yo soy una manzana. El halcón de esta aplicación es muy aterrador.'\n"
     ]
    }
   ],
   "source": [
    "# Nueva copia del modelo\n",
    "lm = llama_lm\n",
    "\n",
    "with system():\n",
    "    lm += \"You are a bot specialized in translating english text to spanish. You always output a literal spanish translation of the user's input.\"\n",
    "\n",
    "with user():\n",
    "    lm += \"This is a pen. I am an apple. This app's owl is very scary.\"\n",
    "\n",
    "with assistant():\n",
    "    lm += gen(name=\"trad_eng\")\n",
    "\n",
    "print(f\"{lm['trad_eng']=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5075b6-6ac8-412e-9959-78e6c10938e8",
   "metadata": {},
   "source": [
    "Con lo anterior podríamos construir de forma sencilla una función de traducción sin necesidad de parseo adicional, pero también es particularmente conveniente para extraer valores numéricos o con patrones claros del modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46186d78-401e-4a55-9ecf-1c6941ec3f0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd1694f2f27f4ebcae42c830fc3dcbf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "StitchWidget(initial_height='auto', initial_width='100%', srcdoc='<!doctype html>\\n<html lang=\"en\">\\n<head>\\n …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El asistente tiene 95 años.\n"
     ]
    }
   ],
   "source": [
    "lm = llama_lm\n",
    "\n",
    "with system():\n",
    "    lm += \"You are a 95-year-old elder.\"\n",
    "\n",
    "with user():\n",
    "    lm += \"How old are you?\"\n",
    "\n",
    "with assistant():\n",
    "    lm += \"I'm \" + gen(name=\"llama_age\", regex=r\"\\d+\") + \" years old.\"\n",
    "\n",
    "print(f\"El asistente tiene {lm['llama_age']} años.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a55ff76-b1e2-4e9a-b72a-d912707a8bc5",
   "metadata": {},
   "source": [
    "Fíjate en que aquí sólo permitimos que el modelo genere una parte muy pequeña del contenido del mensaje, y el resto viene dado por nosotros directamente. Todo mecanismo adicional que introduzcamos para limitar el alcance de las intervenciones del modelo ayudará a mejorar su consistencia.\n",
    "\n",
    "En otras ocasiones, nos interesa ser capaces de generar contenido diferente con cada llamada al modelo para añadir cierto grado de creativididad. Podemos incrementar la aleatoreidad de nuestro LLM por medio del parámetro `temperature`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1dc0f8e6-2369-4338-bb98-c20f83036dfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "722e0b9b65134ff497a0f3b350fc4876",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "StitchWidget(initial_height='auto', initial_width='100%', srcdoc='<!doctype html>\\n<html lang=\"en\">\\n<head>\\n …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 847.\n",
      "[2] 827.\n",
      "[3] 847.\n",
      "[4] 874.\n",
      "[5] 827.\n",
      "[6] 817.\n",
      "[7] 527.\n",
      "[8] 827.\n",
      "[9] 547.\n",
      "[10] 527.\n"
     ]
    }
   ],
   "source": [
    "lm = llama_lm\n",
    "\n",
    "with system():\n",
    "    lm += \"\\nYou are a random number generator.\"\n",
    "\n",
    "with user():\n",
    "    lm += \"Generate a random number between 0 and 1000.\"\n",
    "\n",
    "with assistant():\n",
    "    for i in range(10):\n",
    "        # Cada una de las iteraciones del bucle va a dar lugar a una copia nueva del modelo.\n",
    "        lm_aux = lm + \"The number is: \" +  gen(name=\"rnd_num\", regex=r\"\\d+\", temperature=0.8)\n",
    "        print(f\"[{i + 1}] {lm_aux['rnd_num']}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ac394a-5a8a-4501-a4b4-8a2350c73d12",
   "metadata": {},
   "source": [
    "Si bien hemos conseguido varios números distintos, es evidente que el modelo tiene cierta preferencia por algunos valores. Es importante siempre tener en cuenta que los modelos están siempre sesgados por sus propias probabilidades tras el entrenamiento. De hecho, si hacemos hover encima del valor generado en el widget de arriba podremos ver la probabilidad con la que el LLM genera el token en ese contexto, en nuestro caso para el 542 tendríamos una probabilidad del `0.257`, ¡más una de cada cuatro veces saldría ese mismo valor!\n",
    "\n",
    "Otro tip rápido es que podemos almacenar variables y generaciones del modelo en listas de forma directa usando el parámetro `List_Append=True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a0306ec-71d7-4d17-8856-6323258ef09e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3881766a35324b2b92d695cbda00f215",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "StitchWidget(initial_height='auto', initial_width='100%', srcdoc='<!doctype html>\\n<html lang=\"en\">\\n<head>\\n …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['251', '477', '938', '17', '662', '819', '421', '193', '278', '514']\n"
     ]
    }
   ],
   "source": [
    "lm = llama_lm\n",
    "\n",
    "with system():\n",
    "    lm += \"You are a random number generator.\"\n",
    "\n",
    "with user():\n",
    "    lm += \"Generate a random list of 10 numbers between 0 and 1000.\"\n",
    "\n",
    "with assistant():\n",
    "    for i in range(10):\n",
    "        # Cada una de las iteraciones del bucle va a dar lugar a una copia nueva del modelo.\n",
    "        lm += f\"{i+1}. \" +  gen(name=\"rnd_nums\", regex=r\"\\d+\", temperature=0.8, list_append=True, suffix=\"\\n\")\n",
    "\n",
    "print(lm['rnd_nums'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d124b089-5dae-4c33-93b1-5c860e51ad5c",
   "metadata": {},
   "source": [
    "**¡OJO!** En este segundo ejemplo no estamos realizando una copia del modelo en cada generación de un número aleatorio, por lo que si no reescribimos el prompt, es posible que simplemente se limite a generar el mismo número una y otra vez por contexto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d33e17a-5649-4f76-99cd-8b9a6e6e4ac6",
   "metadata": {},
   "source": [
    "A menudo estaremos interesados en limitar el output del asistente a un conjunto de opciones discretas. Guidance proporciona un método de conveniencia para este caso de uso en `select()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72ca564e-91c6-4796-ac40-5535bea88f7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5517e6521fc14b7fae71343195b19dd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "StitchWidget(initial_height='auto', initial_width='100%', srcdoc='<!doctype html>\\n<html lang=\"en\">\\n<head>\\n …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El modelo seleccionó: Berlin\n"
     ]
    }
   ],
   "source": [
    "from guidance import select\n",
    "\n",
    "lm = llama_lm\n",
    "\n",
    "with system():\n",
    "    lm += \"You are an expert in Geography.\"\n",
    "\n",
    "with user():\n",
    "    lm += \"\"\"Which of the following is the capital of Germany?\n",
    "    A) Paris\n",
    "    B) Madrid\n",
    "    C) Berlin\n",
    "    D) London\n",
    "    \"\"\"\n",
    "\n",
    "with assistant():\n",
    "    lm += select([\"Paris\", \"Madrid\", \"Berlin\", \"London\"], name=\"answer\")\n",
    "\n",
    "print(f\"El modelo seleccionó: {lm['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63478106-6fa6-4f79-9bd5-cb43594a8507",
   "metadata": {},
   "source": [
    "Si bien esto es muy útil, siempre hay que tener sumo cuidado con las halucinaciones, especialmente cuando no incluímos ningún mecanismo de consulta en bases de datos/ textos, etc. Por ejemplo, nuestro modelo falla en la siguiente pregunta, inventando razones falsas para argumentar su respuesta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "097ccfa6-c17b-4017-a12d-e88d76beaaf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20674056afc54439be337e3dca35bb28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "StitchWidget(initial_height='auto', initial_width='100%', srcdoc='<!doctype html>\\n<html lang=\"en\">\\n<head>\\n …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lm = llama_lm\n",
    "\n",
    "with system():\n",
    "    lm += \"You are an expert in videogames.\"\n",
    "\n",
    "with user():\n",
    "    lm += \"\"\"Which of the following is a real The Legend of Zelda game? For each of the answers, indicate whether it is correct or incorrect, as well as the reason for it.\n",
    "\n",
    "    A) The Legend of Zelda: A Link to the Future\n",
    "    B) The Legend of Zelda: Link's Slumber \n",
    "    C) Link's Bowgun Training\n",
    "    D) Ocarina Simulator DSiWare Game\n",
    "    \"\"\"\n",
    "\n",
    "with assistant():\n",
    "    for i in ['A', 'B', 'C', 'D']:\n",
    "        lm += \"Answer \" + i + \" is \" + select(['correct', 'incorrect'], name=i+'resp')\n",
    "        lm += \" for the following reason: \" + gen(stop=[\".\", \"\\n\"], name=i+'reason') + \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa5d213-1047-4a85-bf34-2895ac62b9d7",
   "metadata": {},
   "source": [
    "Muchos de los ejemplos que hemos estado probando hasta el momento se beneficiarían de poder ser transformados en funciones reutilizables en distintos casos de uso y contextos en las que tengan sentido. Vamos a escribir un ejemplo genérico para responder a preguntas de tipo test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe99106b-bc2d-428d-b237-05aa4b913231",
   "metadata": {},
   "outputs": [],
   "source": [
    "import guidance\n",
    "\n",
    "from guidance.models import Model\n",
    "\n",
    "ASCII_OFFSET = ord(\"a\")\n",
    "\n",
    "# el decorador guidance indica que una función puede interactuar con modelos de lenguaje.\n",
    "# por defecto, esto es equivalente a escribir @guidance(stateless=False), lo que quiere\n",
    "# decir que la función altera el estado del modelo (y en este caso de hecho almacena el resultado\n",
    "# de la operación en una variable del mismo). Veremos más adelante un caso de uso típico para\n",
    "# funciones stateless.\n",
    "@guidance\n",
    "def zero_shot_multiple_choice(\n",
    "    language_model: Model,\n",
    "    question: str,\n",
    "    choices: list[str],\n",
    "):\n",
    "    with user():\n",
    "        language_model += question + \"\\n\"\n",
    "        for i, choice in enumerate(choices):\n",
    "            language_model += f\"{chr(i+ASCII_OFFSET)} : {choice}\\n\"\n",
    "\n",
    "    with assistant():\n",
    "        language_model += select(\n",
    "            [chr(i + ASCII_OFFSET) for i in range(len(choices))], name=\"string_choice\"\n",
    "        )\n",
    "\n",
    "    return language_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30b4cd86-d47b-485d-a040-35555ef5d231",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    {\n",
    "        \"question\": \"Which of the following is a traditional Spanish dish made with rice and seafood?\",\n",
    "        \"choices\": [\n",
    "            \"Gazpacho\",\n",
    "            \"Tortilla\",\n",
    "            \"Paella\",\n",
    "            \"Fabada\",\n",
    "            \"Churros\",\n",
    "        ],\n",
    "        \"answer\": 2,  # Paella\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Which of the following animals is venomous?\",\n",
    "        \"choices\": [\n",
    "            \"Iberian scorpion\",\n",
    "            \"Iberian lynx\",\n",
    "            \"Spanish imperial eagle\",\n",
    "        ],\n",
    "        \"answer\": 0,  # Iberian scorpion\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6466692a-96ff-42bf-9fc1-e07030e56402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be8854fcf9334709a5144b9a8728f164",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "StitchWidget(initial_height='auto', initial_width='100%', srcdoc='<!doctype html>\\n<html lang=\"en\">\\n<head>\\n …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "\n",
      "You are a student taking a multiple choice test.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Which of the following is a traditional Spanish dish made with rice and seafood?\n",
      "a : Gazpacho\n",
      "b : Tortilla\n",
      "c : Paella\n",
      "d : Fabada\n",
      "e : Churros\n",
      "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "c\n",
      "LM Answer: 2,  Correct Answer: 2\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "\n",
      "You are a student taking a multiple choice test.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Which of the following animals is venomous?\n",
      "a : Iberian scorpion\n",
      "b : Iberian lynx\n",
      "c : Spanish imperial eagle\n",
      "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "a\n",
      "LM Answer: 0,  Correct Answer: 0\n"
     ]
    }
   ],
   "source": [
    "lm = llama_lm\n",
    "\n",
    "with system():\n",
    "    lm += \"You are a student taking a multiple choice test.\"\n",
    "\n",
    "for mcq in questions:\n",
    "    lm_temp = lm + zero_shot_multiple_choice(question=mcq[\"question\"], choices=mcq[\"choices\"])\n",
    "    converted_answer = ord(lm_temp[\"string_choice\"]) - ASCII_OFFSET\n",
    "    print(lm_temp)\n",
    "    print(f\"LM Answer: {converted_answer},  Correct Answer: {mcq['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e901be3f-4207-42be-a315-548cc8899745",
   "metadata": {},
   "source": [
    "Si queremos construir una función para traducir texto ocultando al usuario el hecho de que está apoyada por un LLM podemos simplemente hacer lo siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52c49aad-d633-411d-9350-699987ed0374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a80ee2a1610d462a84c625ee1be5af81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "StitchWidget(initial_height='auto', initial_width='100%', srcdoc='<!doctype html>\\n<html lang=\"en\">\\n<head>\\n …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a text that is going to be translated to English using an unknown method.\n"
     ]
    }
   ],
   "source": [
    "def translate_from_spanish(text):\n",
    "    lm = llama_lm\n",
    "    \n",
    "    with system():\n",
    "        lm += \"Eres un bot traductor de español a inglés. Tu output es siempre el input del usuario traducido a inglés de forma literal.\"\n",
    "    \n",
    "    with user():\n",
    "        lm += text\n",
    "    \n",
    "    with assistant():\n",
    "        lm += gen(name=\"trad_eng\")\n",
    "    \n",
    "    return lm['trad_eng']\n",
    "\n",
    "print(translate_from_spanish(\"Esto es un texto que va a ser traducido al inglés usando un método desconocido.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1d48c5-a9b1-4f32-8569-7f4fa0844251",
   "metadata": {},
   "source": [
    "## Generando outputs estructurados mediante gramáticas\n",
    "\n",
    "Las funciones de guidance pueden componerse para construir una gramática libre de contexto. Vamos a ver cómo hacer esto para un ejemplo sencillo de expresiones aritméticas. En particular, vamos a implementar la siguiente gramática:\n",
    "\n",
    "```\n",
    "Expr → Expr + Term | Expr - Term | Term  \n",
    "Term → Term * Factor | Term / Factor | Factor  \n",
    "Factor → (Expr) | number\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37b3fc4a-c2a3-4e8c-9d86-1b08b73e6223",
   "metadata": {},
   "outputs": [],
   "source": [
    "@guidance(stateless=True)\n",
    "def _gen_number(lm: Model):\n",
    "    # genera un número decimal, positivo o negativo\n",
    "    return lm + gen(regex=r\"^-?\\d+(\\.\\d+)?$\") \n",
    "\n",
    "@guidance(stateless=True)\n",
    "def _gen_factor(lm: Model):\n",
    "    # declaramos las opciones disponibles para la derivación\n",
    "    lm += select(\n",
    "        options=[\"( \" + _gen_expr() + \" )\", _gen_number()]\n",
    "    )\n",
    "    return lm\n",
    "\n",
    "@guidance(stateless=True)\n",
    "def _gen_term(lm: Model):\n",
    "    # declaramos las opciones disponibles para la derivación\n",
    "    lm += select(\n",
    "        options=[_gen_term() + \" × \" + _gen_factor(), _gen_term() + \" ÷ \" + _gen_factor(), _gen_factor()]\n",
    "    )\n",
    "    return lm\n",
    "    \n",
    "@guidance(stateless=True)\n",
    "def _gen_expr(lm: Model):\n",
    "    # declaramos las opciones disponibles para la derivación\n",
    "    lm += select(\n",
    "        options=[_gen_expr() + \" + \" + _gen_term(), _gen_expr() + \" - \" + _gen_term(), _gen_term()]\n",
    "    )\n",
    "    return lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db41053b-e9ec-46fb-ab21-1b67d3f80a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from guidance.library import capture, with_temperature\n",
    "\n",
    "@guidance(stateless=True)\n",
    "def make_arithmetic(\n",
    "    lm,\n",
    "    name: str | None = None,\n",
    "    *,\n",
    "    temperature: float = 0.0,\n",
    "):\n",
    "    return lm + capture(\n",
    "        with_temperature(_gen_expr(), temperature=temperature),\n",
    "        name=name,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a19c4167-ca71-451b-8a43-99e9b35ce673",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31a3dada637745ac9cc30ba6909c11b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "StitchWidget(initial_height='auto', initial_width='100%', srcdoc='<!doctype html>\\n<html lang=\"en\">\\n<head>\\n …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lm = llama_lm\n",
    "\n",
    "with system():\n",
    "    lm += \"You are an arithmetic expression generator\"\n",
    "\n",
    "with user():\n",
    "    lm += \"Create an arithmetic expression to represent: 'three point twenty five times the result of adding negative four and sixty, minus one divided by nine'\"\n",
    "\n",
    "with assistant():\n",
    "    lm += make_arithmetic(name=\"arithmetic\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013fcabe-e563-4c4f-a47c-2c6267af3270",
   "metadata": {},
   "source": [
    "**¡Cuidado!** Los LLMs pueden generar tokens para intentar representar caracteres que luego no se pintan bien (dependiendo del modelo que uses puede darse ese caso con la división de arriba).\n",
    "\n",
    "El ejemplo anterior es más nicho, pero podemos seguir la misma filosofía para construir generadores de estructuras más complejas que puedan ser expresadas mediante gramáticas libres de contexto. Vamos a incluir aquí el ejemplo de un generador de HTML sencillo con la siguiente gramática reducida:\n",
    "\n",
    "```\n",
    "<html> ::= \"<html>\\n\" <head> <body> \"</html>\\n\"\n",
    "<head> ::= \"<head>\\n\" <title> \"</head>\\n\"\n",
    "<title> ::= \"<title>\" <text> \"</title>\\n\"\n",
    "<body> ::= \"<body>\\n\" <section>+ \"</body>\\n\"\n",
    "<section> ::= <heading> | <paragraph>+\n",
    "<heading> ::= \"<h1>\" <text> \"</h1>\\n\"\n",
    "           | \"<h2>\" <text> \"</h2>\\n\"\n",
    "           | \"<h3>\" <text> \"</h3>\\n\"\n",
    "<paragraph> ::= \"<p>\" <paragraph-content> \"</p>\\n\"\n",
    "<paragraph-content> ::= <element>+\n",
    "<element> ::= <text>\n",
    "           | \"<em>\" <text> \"</em>\"\n",
    "           | \"<strong>\" <text> \"</strong>\"\n",
    "           | \"<br />\"\n",
    "<text> ::= secuencia de caracteres que haga match con [^<>]+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c12aa00-1902-47c0-94c2-cc9c516b8782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <text> ::= secuencia de caracteres que haga match con [^<>]+\n",
    "@guidance(stateless=True)\n",
    "def _gen_text(lm: Model):\n",
    "    return lm + gen(regex=\"[^<>]+\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d7bc65b5-bbd0-4632-8ca1-c0aa63480567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fíjate cómo aquí en vez de escribir una función para cada una de las posibles derivaciones de tipo <tag>text</tag>,\n",
    "# optamos por crear una única función que acepte el parámetro que represente la etiqueta a utilizar (al fin y al cabo,\n",
    "# todas van a seguir la misma estructura).\n",
    "@guidance(stateless=True)\n",
    "def _gen_text_in_tag(lm: Model, tag: str):\n",
    "    lm += f\"<{tag}>\"\n",
    "    lm += _gen_text()\n",
    "    lm += f\"</{tag}>\"\n",
    "    return lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac6b4890-be5b-4c16-bee0-898ce5161730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y aquí tenemos un ejemplo de uso de la función anterior para el caso de <title>\n",
    "# en la regla: <head> ::= \"<head>\\n\" <title> \"</head>\\n\"\n",
    "@guidance(stateless=True)\n",
    "def _gen_header(lm: Model):\n",
    "    lm += \"<head>\\n\"\n",
    "    lm += _gen_text_in_tag(\"title\") + \"\\n\"\n",
    "    lm += \"</head>\\n\"\n",
    "    return lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eb32bb73-1779-4874-8574-22d782f27bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from guidance.library import one_or_more\n",
    "\n",
    "@guidance(stateless=True)\n",
    "def _gen_heading(lm: Model):\n",
    "    # ahora podemos recurrir al select como hacíamos con las expresiones aritméticas, para indicar todos\n",
    "    # los tipos de headings que soportamos. Fíjate en que no hace falta pasar como parámetro el modelo,\n",
    "    # ya que guidance se encarga de inyectarlo por nosotros en llamadas con su decorador.\n",
    "    lm += select(\n",
    "        options=[_gen_text_in_tag(\"h1\"), _gen_text_in_tag(\"h2\"), _gen_text_in_tag(\"h3\")]\n",
    "    )\n",
    "    lm += \"\\n\"\n",
    "    return lm\n",
    "\n",
    "# <paragraph> ::= \"<p>\" <paragraph-content> \"</p>\\n\"\n",
    "# <paragraph-content> ::= <element>+\n",
    "@guidance(stateless=True)\n",
    "def _gen_para(lm: Model):\n",
    "    lm += \"<p>\"\n",
    "    # algo nuevo aquí es el uso de one_or_more que, como su nombre indica, se encarga de \n",
    "    # permitir que el modelo genere secuencialmente al menos una instancia de la subgramática\n",
    "    # pasada como parámetro, y tantas como considere oportuno de ahí en adelante.\n",
    "    # Esto es útil para definir los párrafos como secuencias arbitrariamente largas\n",
    "    # de texto y/o etiquetas selectas con texto dentro.\n",
    "    lm += one_or_more(\n",
    "        select(\n",
    "            options=[\n",
    "                _gen_text(),\n",
    "                _gen_text_in_tag(\"em\"),\n",
    "                _gen_text_in_tag(\"strong\"),\n",
    "                \"<br />\",\n",
    "            ],\n",
    "        )\n",
    "    )\n",
    "    lm += \"</p>\\n\"\n",
    "    return lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b21903e7-243d-46a5-a187-979994b8b044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y ahora la gramática del cuerpo.\n",
    "# <body> ::= \"<body>\\n\" <section>+ \"</body>\\n\"\n",
    "# <section> ::= <heading> | <paragraph>+\n",
    "@guidance(stateless=True)\n",
    "def _gen_body(lm: Model):\n",
    "    lm += \"<body>\\n\"\n",
    "    lm += one_or_more(select(options=[_gen_heading(), one_or_more(_gen_para())]))\n",
    "    lm += \"</body>\\n\"\n",
    "    return lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd52ce04-3b48-43cd-b1ff-a3a05f81727e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finalmente llegamos a la gramática del HTML en sí desde la raíz.\n",
    "# <html> ::= \"<html>\\n\" <head> <body> \"</html>\\n\"\n",
    "@guidance(stateless=True)\n",
    "def _gen_html(lm: Model):\n",
    "    lm += \"<html>\\n\"\n",
    "    lm += _gen_header()\n",
    "    lm += _gen_body()\n",
    "    lm += \"</html>\\n\"\n",
    "    return lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a8404a5c-dc5b-44a9-bb31-96d16fafdb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Como hicimos con la aritmética, podemos definir un wrapper para que llamar a generar HTML sea un poco\n",
    "# más conveniente, y además podamos incluir variables para recuperar el output y temperatura para variedad.\n",
    "from guidance.library import capture, with_temperature\n",
    "\n",
    "@guidance(stateless=True)\n",
    "def make_html(\n",
    "    lm,\n",
    "    name: str | None = None,\n",
    "    *,\n",
    "    temperature: float = 0.0,\n",
    "):\n",
    "    return lm + capture(\n",
    "        with_temperature(_gen_html(), temperature=temperature),\n",
    "        name=name,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0f161218-1218-47a7-894a-73d016b77112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c56a6048f705404da28dd43254be0aca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "StitchWidget(initial_height='auto', initial_width='100%', srcdoc='<!doctype html>\\n<html lang=\"en\">\\n<head>\\n …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lm = llama_lm\n",
    "\n",
    "with system():\n",
    "    lm += \"You are an expert in generating HTML\"\n",
    "\n",
    "with user():\n",
    "    lm += \"Create a simple and short page telling the history of HTML.\"\n",
    "\n",
    "with assistant():\n",
    "    lm += make_html(name=\"html_story\", temperature=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2035b460-49d6-41aa-89ec-4325b1beb25f",
   "metadata": {},
   "source": [
    "# Generando output en formato JSON\n",
    "\n",
    "Una de las cosas que más suele interesar generar con LLMs (y de las que más quebraderos de cabeza dan a la hora de garantizar que el modelo produce output bien formado) es... ¡JSON! Y lo bueno en este caso es que podemos utilizar todo lo que hemos estado explorando anteriormente a este caso de uso sin mucho esfuerzo adicional, simplemente dándonos cuenta de que un esquema JSON no deja de ser una gramática libre de contexto. Ahora, hacer lo anterior para cada esquema que nos gustaría soportar en nuestra aplicación sería extremadamente tedioso, por lo que guidance ofrece un método de conveniencia especialmente destinado a generar JSON a partir de esquemas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4021b795-3920-4883-a403-75ae8632333d",
   "metadata": {},
   "source": [
    "A continuación vamos a utilizar esta funcionalidad para definir generadores de personajes y de interacciones entre ellos en formatos fácilmente integrables en un entorno de juego estilo novela visual. Empecemos por la generación de personajes a partir de un esquema sencillo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ac46bd78-74d9-4bd9-ad0d-6f48cb006e75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "160cd94980a9452e8f5d2fc284e29c24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "StitchWidget(initial_height='auto', initial_width='100%', srcdoc='<!doctype html>\\n<html lang=\"en\">\\n<head>\\n …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "from pydantic import BaseModel, Field, constr\n",
    "\n",
    "from guidance import json as gen_json\n",
    "\n",
    "# prueba a quitar esta restricción para ver cómo el modelo empieza a explayarse hasta agotar el espacio...\n",
    "DescriptionStr = constr(max_length=200, pattern=r\".*\\.$\")\n",
    "\n",
    "# modelo de pydantic para definir un personaje\n",
    "class Character(BaseModel):\n",
    "    name: str = Field(max_length=20)\n",
    "    description: DescriptionStr = Field(...)\n",
    "    age: int = Field(gt=0, le=100)\n",
    "\n",
    "    class Config:\n",
    "        # parámetros adicionales para limpiar finales de strings y, más importantemente,\n",
    "        # para evitar que el LLM añada campos no necesarios en su output.\n",
    "        # Esto es relevante porque, por defecto, un esquema sólo exige la presencia de ciertos\n",
    "        # campos, pero que un objeto tenga campos adicionales no invalida la especificación.\n",
    "        # En generación, esto puede traducirse en un incremento significativo del coste\n",
    "        # en tiempo para producir JSON, y además objetos con un exceso de datos inútiles.\n",
    "        extra = 'forbid'\n",
    "        str_strip_whitespace = True\n",
    "\n",
    "\n",
    "lm = llama_lm\n",
    "\n",
    "with system():\n",
    "    lm += \"You are a fantasy game designer focused on creating interesting yet brief character descriptions and narratives\"\n",
    "\n",
    "with user():\n",
    "    n = 3\n",
    "    lm += f\"Create {n} characters for a disney-inspired RPG game. Each field must be one line long at most.\"\n",
    "\n",
    "characters = []\n",
    "with assistant():\n",
    "    for i in range(n):\n",
    "        lm += gen_json(name=f\"character_{i}\", schema=Character)\n",
    "        # convertimos el string generado en un objeto de Python validado por su esquema\n",
    "        result = Character.model_validate_json(lm[f\"character_{i}\"])\n",
    "        characters.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9296f85f-5dfc-4e97-bb17-e4980291f77d",
   "metadata": {},
   "source": [
    "Ahora que tenemos nuestros personajes, podemos pasar a crear una historia que haga uso de ellos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "afb873b4-df06-45d7-ac8e-e6ddb8a8be7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c015a04109846c8890d28a9e931e92f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "StitchWidget(initial_height='auto', initial_width='100%', srcdoc='<!doctype html>\\n<html lang=\"en\">\\n<head>\\n …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from typing import Literal\n",
    "from pydantic import field_validator, conlist\n",
    "\n",
    "# tenemos la garantía de que los personajes están bien formados.\n",
    "character_names = [c.name for c in characters]\n",
    "\n",
    "class CharacterIntervention(BaseModel):\n",
    "    character_name: str\n",
    "    intervention_text: str = Field(max_length=200)\n",
    "\n",
    "    class Config:\n",
    "        extra = 'forbid'\n",
    "        str_strip_whitespace = True\n",
    "\n",
    "    # este es un ejemplo de cómo se pueden utilizar validadores también para guiar el output.\n",
    "    # en este caso, estamos interesados en garantizar que el nombre del personaje referenciado\n",
    "    # en cada una de las interacciones sea válido y esté incluído en la lista creada en el \n",
    "    # paso anterior. Esto no es necesariamente una buena práctica y de hecho el modelo no \n",
    "    # debería ser tan específico en situaciones más generales, pero aquí lo hacemos así\n",
    "    # a efectos de demostrar que se pueden hacer cosas como esta.\n",
    "    @field_validator('character_name')\n",
    "    def name_must_be_valid(cls, v):\n",
    "        if v not in character_names:\n",
    "            raise ValueError(f\"character_name must be one of {character_names}\")\n",
    "        return v\n",
    "    \n",
    "\n",
    "class Narrative(BaseModel):\n",
    "    # otro ejemplo de algo que se puede hacer: establecer el rango de elementos que puede \n",
    "    # incluir una lista. De nuevo, como en el punto anterior, esto seguramente aquí no sea\n",
    "    # muy adecuado si nuestra intención fuera generalizar, pero lo hacemos para demostrar\n",
    "    # que se puede generar el JSON entero puramente a través del esquema y la declaración.\n",
    "    interventions: conlist(CharacterIntervention, min_length=4, max_length=6)\n",
    "\n",
    "    class Config:\n",
    "        extra = 'forbid'\n",
    "        str_strip_whitespace = True\n",
    "\n",
    "\n",
    "# continuamos con el mismo modelo de antes (obviamos el lm = llama_lm).\n",
    "\n",
    "with user():\n",
    "    lm += f\"Create a narrative dialogue between those characters.\"\n",
    "\n",
    "with assistant():\n",
    "    lm += gen_json(name=f\"narrative\", schema=Narrative)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2154e25-e075-4258-a16b-fcbdd5043f2c",
   "metadata": {},
   "source": [
    "## Utilizando tools\n",
    "\n",
    "El último bloque de funcionalidad de interés es la capacidad de guidance de utilizar tools externas para darle al modelo la capacidad de solicitar la realización de operaciones complejas, llamadas a APIs, o consultas a bases de datos de forma directa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "88dc8560-f9c6-4e77-869c-98e411698902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfe0b10edbfe46ef9c54c46890c77d67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "StitchWidget(initial_height='auto', initial_width='100%', srcdoc='<!doctype html>\\n<html lang=\"en\">\\n<head>\\n …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@guidance\n",
    "def add(lm, input1, input2):\n",
    "    lm += f' = {int(input1) + int(input2)}'\n",
    "    return lm\n",
    "\n",
    "@guidance\n",
    "def subtract(lm, input1, input2):\n",
    "    lm += f' = {int(input1) - int(input2)}'\n",
    "    return lm\n",
    "\n",
    "@guidance\n",
    "def multiply(lm, input1, input2):\n",
    "    lm += f' = {float(input1) * float(input2)}'\n",
    "    return lm\n",
    "\n",
    "@guidance\n",
    "def divide(lm, input1, input2):\n",
    "    lm += f' = {float(input1) / float(input2)}'\n",
    "    return lm\n",
    "\n",
    "lm = llama_lm\n",
    "\n",
    "with system():\n",
    "    lm += \"You are a calculator bot that does not speak and only calculates.\"\n",
    "    \n",
    "with user():\n",
    "    lm += \"Continue the following sequence of operations.\\n\"\n",
    "    lm += \"\"\"\n",
    "    1 + 1 = add(1, 1) = 2\n",
    "    2 - 3 = subtract(2, 3) = -1\n",
    "    \"\"\"\n",
    "    \n",
    "with assistant():  \n",
    "    lm += \"5 + 6 =\"\n",
    "    lm += gen(max_tokens=15, stop=\"\\n\", tools=[add, subtract, multiply, divide]) + \"\\n\"\n",
    "    lm += \"8 - 98 =\"\n",
    "    lm += gen(max_tokens=15, stop=\"\\n\", tools=[add, subtract, multiply, divide]) + \"\\n\"\n",
    "    lm += \"5 * 6 =\"\n",
    "    lm += gen(max_tokens=15, stop=\"\\n\", tools=[add, subtract, multiply, divide]) + \"\\n\"\n",
    "    lm += \"40 / 9 =\"\n",
    "    lm += gen(max_tokens=15, stop=\"\\n\", tools=[add, subtract, multiply, divide]) + \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7ce11a-9e9f-42a7-9186-040b27142828",
   "metadata": {},
   "source": [
    "El ejemplo anterior, como ocurría con la calculadora, simplemente recurre a funciones de Python para incrementar la precisión del modelo y evitar que \"prediga\" algo que es fácil de resolver por código. No obstante, un caso de uso interesante es permitir que el modelo realice llamadas a APIs o bases de datos externas para acceder a información real que pueda incluir en su razonamiento, de nuevo evitando que halucine con predicciones erróneas.\n",
    "\n",
    "En el siguiente ejemplo sencillo vamos a ver cómo utilizar tools para consultar una API muy tonta que va a permitir al modelo consultar el tiempo atmosférico para tomar decisiones informadas sobre dónde viajar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0d7a1299-ba8f-43a1-a144-be6fe9cf895b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "London: ☀️   +25°C\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "city = \"London\"\n",
    "url = f\"https://wttr.in/{city}?format=3\"  # formato sencillo: City: weather, temp\n",
    "\n",
    "response = requests.get(url)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4f37518b-d044-460a-b49d-bec4d91b87ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6486d90902594c24861043ada570a880",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "StitchWidget(initial_height='auto', initial_width='100%', srcdoc='<!doctype html>\\n<html lang=\"en\">\\n<head>\\n …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "@guidance\n",
    "def check_weather_in(lm, city):\n",
    "    # nos limitamos a extraer el lugar y la temperatura para evitar el caracter con el estado de lluvias.\n",
    "    url = f\"https://wttr.in/{city}?format=3\"\n",
    "    response = requests.get(url)\n",
    "    match = re.match(r\"([^:]+):.*?([+-]?\\d+°C)\", response.text)\n",
    "    if match:\n",
    "        clean_output = f\"{match.group(1)}: {match.group(2)}\"\n",
    "        lm += f'= {clean_output}'\n",
    "    else:\n",
    "        lm += \"Could not parse weather data.\"\n",
    "    return lm\n",
    "\n",
    "lm = llama_lm\n",
    "\n",
    "with system():\n",
    "    lm += \"You are a helpful reasoning agent. You solve problems by thinking step-by-step and using tools when needed.\"\n",
    "\n",
    "with user():\n",
    "    lm += (\n",
    "        \"Answer the following question by reasoning step-by-step.\\n\"\n",
    "        \"Use the format:\\n\"\n",
    "        \"Question: {question}\\n\"\n",
    "        \"Thought: {your reasoning}\\n\"\n",
    "        \"Action: {tool name}(input)\\n\"\n",
    "        \"Observation: {result from tool (check_weather_in(...))}\\n\"\n",
    "        \"... (repeat Thought/Action/Observation as needed)\\n\"\n",
    "        \"Thought: {final reasoning}\\n\"\n",
    "        \"END\\n\"\n",
    "        \"Answer: {final answer}\\n\\n\"\n",
    "        \"Begin!\\n\\n\"\n",
    "        \"Question: Please find a European city where temperatures are under 25 degrees now. In Madrid, it's check_weather_in(Madrid) = Madrid: +30°C. Too hot!\"\n",
    "    )\n",
    "\n",
    "with assistant():\n",
    "    lm += gen(\n",
    "        max_tokens=10000,\n",
    "        stop=\"END\",\n",
    "        tools=[check_weather_in]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11715617-9eb2-4731-b1d6-52193d686435",
   "metadata": {},
   "source": [
    "El patrón de prompting que hemos usado en la celda anterior se conoce como [ReAct Prompting](https://www.promptingguide.ai/techniques/react), un enfoque que permite a los LLMs intercalar razonamientos y acciones para planificar, adaptarse y consultar fuentes externas. Cuando esto funciona como debe, es un buen recurso para mejorar la precisión y fiabilidad de las respuestas de un chatbot. Existen muchísimas estrategias de Prompting con objetivos muy diversos. Como nos eternizaríamos hablando de todas, para un overview puedes consultar [esta referencia](https://www.promptingguide.ai/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tutorial_llms_poetry)",
   "language": "python",
   "name": "tutorialllms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
